import json
from bs4 import BeautifulSoup
import requests
import re
import nltk
import lightgbm as lgb
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.data.path.append("/tmp")
nltk.data.path.append(".")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective/sentiment")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective/sentiment/vader_lexicon")

nltk.download("punkt", download_dir="/tmp")

sia = SentimentIntensityAnalyzer(lexicon_file='vader_lexicon/vader_lexicon.txt')

# knowledge_model_file = 'knowledge_model.txt'
# usefulness_model_file = 'usefulness_model.txt'
vectorizer_file = 'pkl/vectorizer.pkl'

# def load_model(model_file):
#     with open(model_file, 'r') as file:
#         model = json.load(file)
#     return model

def load_vectorizer(vectorizer_file):
    with open(vectorizer_file, 'r') as file:
        vectorizer = json.load(file)
    return vectorizer

# knowledge_model = load_model(knowledge_model_file)
# usefulness_model = load_model(usefulness_model_file)
vectorizer = load_vectorizer(vectorizer_file)
knowledge_model = lgb.Booster(model_file="lightgbm_models/Knowledgelightgbm_model.txt")
usefulness_model = lgb.Booster(model_file ="lightgbm_models/Actionlightgbm_model.txt")


stop_words = set(stopwords.words('english'))

# Define the regular expression to use for tokenization
token_pattern = r'\b\w+\b'

def get_data(url: str) -> str:
    """
    Gets text from given url

    Args: 
        url (str): url to grab text from.

    Returns:
        str: text found at 'http://[url]', where '[url]' is replaced by the value of url
    """
    # Prepend http:// to url if it isn't there
    url = re.sub('https?://', '', url)
    r = requests.get(f'http://{url}')
    return r.text

def text_preprocessing(text: str) -> str:
    """
    Preprocesses text before being analyzed for score prediction by ML.
    Removes the following:
        * all words that begin with '@'
        * all links starting with 'https://' or 'http://'
        * all words that begin with a number
        * all words that begin with '#' 

    Args:
        text (str): text to preprocess according to the rules above

    Returns: 
        str: preprocessed text according to the rules above
    """
    text = text.lower()
    text = re.sub("@\\w+", "", text)
    text = re.sub("https?://.+", "", text)
    text = re.sub("\\d+\\w*\\d*", "", text)
    text = re.sub("#\\w+", "", text)
    return text

def analyze(url: str):
    """
    Grabs the text from 'url', gets all paragraphs within the website, runs analysis to get three scores, 
    and returns the scores. These scores are:
        * sentiment
        * actionability
        * knowledge

    The scores range from -1 to 1, inclusive.
    
    Args:
        url (str): url to grab text from

    Returns:
        dict: a dictionary that stores the url and the evaluated scores
    """
    scores = {
        'emotion': -1,
        'usefulness': -1,
        'knowledge': -1
    }

    try:
        html = get_data(url)
        soup = BeautifulSoup(html, 'html.parser')

        paragraph = []
        # Iterate through all paragraphs in website, preprocess, tokenize, and filter words
        for data in soup.find_all("p"):
            sentence = data.get_text()

            sentence = text_preprocessing(sentence)

            text_tokens = word_tokenize(sentence)

            tokens_without_sw = [word for word in text_tokens if not word in stop_words]
            filtered_sentence = " ".join(tokens_without_sw)
            paragraph.append(filtered_sentence)

            # If text gets too long, break out of loop
            if len(paragraph) > 500:
                break
        paragraph = " ".join(paragraph)

        # Calculate scores
        scores['emotion'] = sia.polarity_scores(paragraph)['compound']
        vectorizer = vectorizer.transform([paragraph])
        scores['knowledge'] = knowledge_model.predict(vectorizer)[0]
        scores['usefulness'] = usefulness_model.predict(vectorizer)[0]
    except Exception as e:
        raise Exception("Failed to analyze link: " + str(e))

    return {
        'link': url,
        'scores': scores,
    }
