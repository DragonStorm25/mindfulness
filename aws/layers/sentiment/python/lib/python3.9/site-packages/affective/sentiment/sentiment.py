from bs4 import BeautifulSoup
import requests
import re
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize

import pickle

def predict_action(text: str):
    """
    Predict the actionability score of a string

    Args:
        text (str): text input to analyze actionability score.

    Returns: 
        int: predicted actionability score of text, within [-1, 1].
    """
    action_log_reg = pickle.load(open('/opt/python/lib/python3.9/site-packages/affective/sentiment/pkl/Action_log_reg.pkl', 'rb'))
    action_vectorizer = pickle.load(open('/opt/python/lib/python3.9/site-packages/affective/sentiment/pkl/Action_vectorizer.pkl', 'rb'))
    X = action_vectorizer.transform([text])
    proba = action_log_reg.predict_proba(X)[:, 1]
    return proba

def predict_knowledge(text: str):
    """
    Predict the knowledge score of a string

    Args:
        text (str): text input to analyze knowledge score.

    Returns: 
        int: predicted knowledge score of text, within [-1, 1].
    """
    knowledge_log_reg = pickle.load(open('/opt/python/lib/python3.9/site-packages/affective/sentiment/pkl/Knowledge_log_reg.pkl', 'rb'))
    knowledge_vectorizer = pickle.load(open('/opt/python/lib/python3.9/site-packages/affective/sentiment/pkl/Knowledge_vectorizer.pkl', 'rb'))
    X = knowledge_vectorizer.transform([text])
    proba = knowledge_log_reg.predict_proba(X)[:, 1]
    return proba

nltk.data.path.append("/tmp")
nltk.data.path.append(".")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective/sentiment")
nltk.data.path.append("/opt/python/lib/python3.9/site-packages/affective/sentiment/vader_lexicon")


nltk.download("punkt", download_dir = "/tmp")
# nltk.download('stopwords', download_dir='/tmp')

"""load the vader lexicon for sentiment analysis"""
sia = SentimentIntensityAnalyzer(lexicon_file='vader_lexicon/vader_lexicon.txt')

stopwords = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 
             'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', 
             "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 
             'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 
             'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 
             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 
             'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 
             'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 
             'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 
             "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 
             'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 
             'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 
             'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"}

def get_data(url: str) -> str:
    """
    Gets text from given url

    Args: 
        url (str): url to grab text from.

    Returns:
        str: text found at 'http://[url]', where '[url]' is replaced by the value of url
    """
    # Prepend http:// to url if it isn't there
    url = re.sub('https?://', '', url)
    r = requests.get(f'http://{url}')
    return r.text

def text_preprocessing(text: str) -> str:
    """
    Preprocesses text before being analyzed for score prediction by ML.
    Removes the following:
        * all words that begin with '@'
        * all links starting with 'https://' or 'http://'
        * all words that begin with a number
        * all words that begin with '#' 

    Args:
        text (str): text to preprocess according to the rules above

    Returns: 
        str: preprocessed text according to the rules above
    """
    text = text.lower()
    text = re.sub("@\\w+", "", text)
    text = re.sub("https?://.+", "", text)
    text = re.sub("\\d+\\w*\\d*", "", text)
    text = re.sub("#\\w+", "", text)
    return(text)

def analyze(url: str):
    """
    Grabs the text from 'url', gets all paragraphs within the website, runs analysis to get three scores, 
    and returns the scores. These scores are:
        * sentiment
        * actionability
        * knowledge

    The scores range from -1 to 1, inclusive.
    
    Args:
        url (str): url to grab text from

    Returns:
        dict: a dictionary that stores the url and the evaluated scores
    """
    scores = {
        'emotion': -1,
        'usefulness': -1,
        'knowledge': -1
    }

    try:
        html = get_data(url)
        soup = BeautifulSoup(html, 'html.parser')
            
        paragraph = []
        # Iterate through all paragraphs in website, preprocess, tokenize, and filter words
        for data in soup.find_all("p"):
            sentence = data.get_text()

            sentence = text_preprocessing(sentence)

            text_tokens = word_tokenize(sentence)

            tokens_without_sw = [word for word in text_tokens if not word in stopwords]
            filtered_sentence = (" ").join(tokens_without_sw)
            paragraph.append(filtered_sentence)
            
            # If text gets too long, break out of loop
            if len(paragraph) > 500:
                break
        paragraph = ' '.join(paragraph)

        # Calculate scores
        emotion_score = sia.polarity_scores(paragraph)['compound']
        scores['emotion'] = emotion_score

        action_score = predict_action(paragraph)
        scores['usefulness'] = action_score

        knowledge_score = predict_knowledge(paragraph)
        scores['knowledge'] = knowledge_score
    except Exception as e:
        raise Exception('Failed to get emotion score for link: ', str(e))

    return {
        'link': url,
        'scores': scores,
    }